import os
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import subprocess

# Caminhos no seu Mac
BASE_MODEL_DIR = "/Users/alvaromra/Desktop/Script_para_docs/models/base-tinyllama"
LORA_DIR = "/Users/alvaromra/Desktop/Script_para_docs/models/minha-lora"
MERGED_DIR = "/Users/alvaromra/Desktop/Script_para_docs/models/merged-hf"
OUTPUT_GGUF = "/Users/alvaromra/Desktop/Script_para_docs/models/minha-lora-merged.gguf"

# Caminho para o conversor do llama.cpp
CONVERT_SCRIPT = "/Users/alvaromra/Desktop/llama.cpp/convert_hf_to_gguf.py"

def main():
    print("üîç Verificando diret√≥rios...")
    if not os.path.isdir(BASE_MODEL_DIR):
        raise SystemExit(f"‚ùå Base model n√£o encontrado: {BASE_MODEL_DIR}")
    if not os.path.isdir(LORA_DIR):
        raise SystemExit(f"‚ùå LoRA n√£o encontrado: {LORA_DIR}")

    print("üì• Carregando modelo base...")
    base = AutoModelForCausalLM.from_pretrained(BASE_MODEL_DIR, device_map="cpu")

    print("üì• Carregando pesos LoRA...")
    lora = PeftModel.from_pretrained(base, LORA_DIR)

    print("üîó Mesclando LoRA ‚Üí Modelo base...")
    merged = lora.merge_and_unload()

    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_DIR)

    Path(MERGED_DIR).mkdir(parents=True, exist_ok=True)

    print(f"üíæ Salvando modelo mesclado em {MERGED_DIR}...")
    merged.save_pretrained(MERGED_DIR)
    tokenizer.save_pretrained(MERGED_DIR)

    print("‚öôÔ∏è Convertendo para GGUF...")
    cmd = [
        "python3",
        CONVERT_SCRIPT,
        MERGED_DIR,
        "--outfile", OUTPUT_GGUF,
        "--outtype", "q8_0",
    ]

    print("üîß Rodando comando:")
    print(" ".join(cmd))

    subprocess.run(cmd, check=True)

    print(f"üéâ GGUF gerado com sucesso em {OUTPUT_GGUF} !")

if __name__ == "__main__":
    main()
